{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/tutorial/random-forests-classifier-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A random forest is a set of randomly constructed decision trees\n",
    "* Each decision tree is constructed from an independent random sample of training data.\n",
    "* In the case of <u>classification</u>, we allow each decision tree to classify individually, and then vote for that classification. The most popular classification is the final output.\n",
    "* In the case of <u>regression</u>, we allow each decision tree to output its own prediction, and then output the average of these predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROS\n",
    "* Since this is a random <u>forest</u> of many decision trees, the model is less sensitive to bias (e.g. over-fitting)\n",
    "* The random forest model can be used in classification <u>and</u> regression, we just have to change between voting and averaging\n",
    "* We can use relative feature importance (feature = exogenous variable) to drop less-contributing features, thereby increasing accuracy\n",
    "\n",
    "CONS\n",
    "* Since we have hundreds of decision trees, each predicting individually and then voting/averaging all together, the model can be slow\n",
    "* It is hard to transform our model into a clear set of rules for classification, since we have hundreds of decision trees (rather than one decision tree that can be easily interpreted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random Forest is an example of an <u>ensemble</u> algorithm: an algorithm that combines multiple models (each decision tree)\n",
    "* There are two methods of ensemble - <u>bagging</u> and <u>boosting</u>. Random Forest uses bagging\n",
    "    * <u>Bagging</u>: create different training subsets from sample training data (with replacement), final prediction/classification comes from voting\n",
    "    * <u>Boosting</u>: a sequential model that combines weak learners to create a strong learner - the final model is the most accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging / Bootstrap Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We begin with the original training data\n",
    "2. We randomly sample with replacement from the original training data to create multiple training subsets, each yields a model - <u>Bootstrapping</u>\n",
    "3. Each training subset is used to create and train a decision tree - <u>Aggregation</u>\n",
    "4. We then vote/average the results of each decision tree using test data to create the final output - <u>Ensemble Classifier</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase Predictive Power\n",
    "1. $n_{estimators}$ = number of trees in the random forest\n",
    "2. $max_{features}$ = max number of exogenous variables considered when splitting a node in each decision tree\n",
    "3. $min_{sample-leaf}$ = minimum number of leaves required to split an internal node\n",
    "\n",
    "Increase Speed\n",
    "1. $n_{jobs}$ = how many processors the engine is allowed to use. If -1, there is no limit\n",
    "2. $random_{state}$ = controls randomness of the sampling process. Given all other parameters/dataset is the same, the model will produce identical results if the same $random_{state}$ is used\n",
    "3. $oob_{score}$ = *Out of the Bag*; one-third of the sample not used to train, but instead to test the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating each Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Indeed, the easy way to implement a random forest algorithm is to import the RandomForestClassifier or RandomForestRegressor from the scikit-learn library in Python. This abstracts the process of creating each individual decision tree.*\n",
    "\n",
    "*That being said, let's make the decision trees ourselves. Who doesn't like a challenge...*\n",
    "\n",
    "So, how does each tree look? Below is a simple example from the titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='./pictures/1_XMId5sJqPtm8-RIwVVz2tg.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each node, we have a feature/exogenous variables: *sex, age, number of siblings/spouses*. Some of these features are discrete (sex) and others continuous (age, sibsp). Depending on the value, we traverse to the next node (internal or leaf) along the corresponding edge. The classification/prediction that the tree outputs is dependent on the leaf node that we arrive at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*That being said, how do we know which features to use as well as the conditions for traversing an edge?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Binary Splitting - A Greedy Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by including all features (exogenous variables) in the decision tree and use various conditions to split each node. We then test these split conditions using a cost function. The split conditions that yield the lowest cost are used in the final tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume there are $k$ features to select from. Therefore, at the root node, we have $k$ options for the feature used to split. We calculate the accuracy cost (using the cost function) for each of the $k$ options, and select the feature with the lowest cost. This is the feature used to split at the root node.\n",
    "\n",
    "We recursively repeat this for each branch in the tree.\n",
    "\n",
    "Because the feature at the root node yielded the lowest cost, the first feature in the tree is the best predictor/classifier in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of <u>Regression</u>: $$Cost = \\sum{(y - \\hat{y})^2}$$ where y is the true value and $\\hat{y}$ is the prediction. \n",
    "\n",
    "An alternative cost function that can be used focuses on minimizing the standard deviation, rather than the squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of <u>Classification</u>: $$G = \\sum{pk(1-pk)}$$\n",
    "\n",
    "$G$ is the gini score of splitting by a certain feature. $pk$ is the proportion of same class inputs within a group, $0 \\leq pk \\leq 1$\n",
    "\n",
    "Say we split by sibsp in the root node. Since sibsp is not as good of a classifier as sex, the proportion of same class inputs will be closer to 0.5. In other words, if we split by sibsp first, the proportion of 'died' vs. 'survived' in the left and right branch will be close to 50/50.\n",
    "\n",
    "Now, assume we split by sex first. Since sex is a better classifier, each branch will have a higher proportion of same-class inputs. In other words, the 'male' branch will have a high proportion of 'died' output and the 'female' branch will have a low proportion of 'died' output.\n",
    "\n",
    "To be mathematical, as $pk$ --> 0 or 1, G --> 0. Moreover, $pk$ --> 0 or 1 if a better classifier is selected (because there will be a higher or lower proportion of the same class, not a 50/50 split). If the classifier is bad, then $pk$ --> 0.5 (50/50 split of 'died' vs. 'survived').\n",
    "\n",
    "Therefore, a lower gini score indicates a better classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Pruning</u> is the removal of branches that are split based on features with low importance. By doing this, we reduce the height of the tree without sacrificing a good classification feature.\n",
    "\n",
    "One method of pruning is <u>Reduced Error Pruning</u>: \n",
    "* We begin at the lowest level internal nodes (nodes that have an associated feature) and work our way up\n",
    "* For each node, we remove the subtree below it, thus making the node a leaf node. This new leaf node is assigned whatever was the most popular class at that node \n",
    "* If the removal of a node's subtree (and re-assignment of the node as a leaf node) does not hurt the performance of the overall tree, the node is pruned.\n",
    "\n",
    "Another method of pruning is *Cost Complexity Pruning*, which uses a learning parameter ($\\alpha$) to determine if a node can be removed based on the size of its sub-tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree to Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are able to construct a decision tree, we randomly sample from the training data (as was described earlier) and construct a decision tree with each strata. Then, all that is left to do is to let each decision tree classify/predict and vote/average. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "213c7e2037ef6b2a20c19d597b02c10a1d7071ecb66c3155fc7058d00ffab593"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
