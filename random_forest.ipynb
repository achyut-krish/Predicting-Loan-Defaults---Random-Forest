{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This project implements a random forest model to predict (classification) whether or not a person will default on their loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Let us begin by reading the Loan Default dataset.\n",
    "\n",
    "The original csv can be found at: https://www.kaggle.com/kmldas/loan-default-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(\"./\", \"Default_Fin.csv\")\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Employed</th>\n",
       "      <th>Bank Balance</th>\n",
       "      <th>Annual Salary</th>\n",
       "      <th>Defaulted?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8754.36</td>\n",
       "      <td>532339.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9806.16</td>\n",
       "      <td>145273.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12882.60</td>\n",
       "      <td>381205.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6351.00</td>\n",
       "      <td>428453.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9427.92</td>\n",
       "      <td>461562.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>1</td>\n",
       "      <td>8538.72</td>\n",
       "      <td>635908.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>1</td>\n",
       "      <td>9095.52</td>\n",
       "      <td>235928.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>1</td>\n",
       "      <td>10144.92</td>\n",
       "      <td>703633.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>1</td>\n",
       "      <td>18828.12</td>\n",
       "      <td>440029.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>2411.04</td>\n",
       "      <td>202355.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index  Employed  Bank Balance  Annual Salary  Defaulted?\n",
       "0         1         1       8754.36      532339.56           0\n",
       "1         2         0       9806.16      145273.56           0\n",
       "2         3         1      12882.60      381205.68           0\n",
       "3         4         1       6351.00      428453.88           0\n",
       "4         5         1       9427.92      461562.00           0\n",
       "...     ...       ...           ...            ...         ...\n",
       "9995   9996         1       8538.72      635908.56           0\n",
       "9996   9997         1       9095.52      235928.64           0\n",
       "9997   9998         1      10144.92      703633.92           0\n",
       "9998   9999         1      18828.12      440029.32           0\n",
       "9999  10000         0       2411.04      202355.40           0\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features: Employed (1 = yes, 0 = no), Bank Balance, Annual Salary\n",
    "# Classification: Defaulted (1 = yes, 0 = no)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 features (employment, bank balance, and annual salary) that can be used to classify whether the person defaults on their loan. \n",
    "\n",
    "To construct the tree, we will need a cost function to determine the best classifier. Since we are doing classification, rather than regression, we can use the Gini Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train and Test\n",
    "Split into test and train data: 70% train. The training data is sampled randomly, without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.7, replace=False)\n",
    "df_test = df.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function: Gini Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function $$G = \\sum{pk(1-pk)}$$ where $G$ is the gini score of splitting by a certain feature, $pk$ is the proportion of same class inputs within a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_score(feature: str, data: pd.DataFrame, split_value):\n",
    "    \"\"\"\n",
    "    This function computes the gini score for splitting data by the given feature.\n",
    "    Given a split value, we split the data into a left and right branch. For \n",
    "    each branch, we count the number of each class in the data. We use these \n",
    "    counts to compute pk for each class, and then the gini score.\n",
    "\n",
    "    If one of the branches is empty, then we compute the gini score only using the \n",
    "    non-empty branch. \n",
    "    \"\"\"\n",
    "    \n",
    "    left_branch = data.loc[data[feature] < split_value]\n",
    "    right_branch = data.loc[data[feature] >= split_value]\n",
    "\n",
    "    left_0 = left_branch.loc[left_branch['Defaulted?'] == 0]['Defaulted?'].count()\n",
    "    left_1 = left_branch.loc[left_branch['Defaulted?'] == 1]['Defaulted?'].count()\n",
    "\n",
    "    right_0 = right_branch.loc[right_branch['Defaulted?'] == 0]['Defaulted?'].count()\n",
    "    right_1 = right_branch.loc[right_branch['Defaulted?'] == 1]['Defaulted?'].count()\n",
    "    \n",
    "    # Left branch is empty, compute pk for each class using right branch only\n",
    "    if(len(left_branch) == 0 and len(right_branch) > 0):\n",
    "        # We compute pk w.r.t. the first class: 0 (did not default on loan)\n",
    "        pk_0_right = right_0 / (right_0 + right_1)\n",
    "        pk_0 = pk_0_right\n",
    "\n",
    "        # We compute pk w.r.t. the second class: 1 (did default on loan)\n",
    "        pk_1_right = right_1 / (right_0 + right_1)\n",
    "        pk_1 = pk_1_right\n",
    "\n",
    "        gini_score = (pk_0*(1-pk_0) + pk_1*(1-pk_1))\n",
    "        return(gini_score)\n",
    "    \n",
    "    # Right branch is empty, compute pk for each class using left branch only\n",
    "    elif(len(right_branch) == 0 and len(left_branch) > 0):\n",
    "        # We compute pk w.r.t. the first class: 0 (did not default on loan)\n",
    "        pk_0_left = left_0 / (left_0 + left_1)\n",
    "        pk_0 = pk_0_left\n",
    "\n",
    "        # We compute pk w.r.t. the secon class: 1 (did default on loan)\n",
    "        pk_1_left = left_1 / (left_0 + left_1)\n",
    "        pk_1 = pk_1_left\n",
    "\n",
    "        gini_score = (pk_0*(1-pk_0) + pk_1*(1-pk_1))\n",
    "        return(gini_score)\n",
    "\n",
    "    # Neither branch is empty, compute pk for each class using both branches\n",
    "    else:\n",
    "        # We compute pk w.r.t. the first class: 0 (did not default on loan)\n",
    "        pk_0_left = left_0 / (left_0 + left_1)\n",
    "        pk_0_right = right_0 / (right_0 + right_1)\n",
    "        pk_0 = np.average([pk_0_left, pk_0_right])\n",
    "\n",
    "        # We compute pk w.r.t. the second class: 1 (did default on loan)\n",
    "        pk_1_left = left_1 / (left_0 + left_1)\n",
    "        pk_1_right = right_1 / (right_0 + right_1)\n",
    "        pk_1 = np.average([pk_1_left, pk_1_right])\n",
    "\n",
    "        gini_score = (pk_0*(1-pk_0) + pk_1*(1-pk_1))\n",
    "        return(gini_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a Classification\n",
    "To generate a classification at a leaf node, we find the most popular class in the sub-group of training data at that leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_class(data):\n",
    "    \"\"\"\n",
    "    This function groups data by each class and gets the counts.\n",
    "    \n",
    "    The function returns the most popular (highest count) class in data.\n",
    "    \"\"\"\n",
    "    # For each class, the counts in each column are the same, so we \n",
    "    # just select the first column\n",
    "    count_df = data.groupby(['Defaulted?']).count()['Index']\n",
    "    return(count_df.idxmax())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(features_list, train, test):\n",
    "    \"\"\"\n",
    "    We begin with a list of all the features available in the data. The possible \n",
    "    values we can use to split the data for each feature are the min, max, and each \n",
    "    quartile. The feature/split-value combination with the lowest gini score is \n",
    "    used to split the training data first. Based on the value of that feature in \n",
    "    our test data relative to the chosen split-value, we move into the corresponding \n",
    "    branch of the tree and recursively repeat. We continue until we have no more \n",
    "    features to split with. \n",
    "\n",
    "    Note: test is one row of test data\n",
    "    \"\"\"\n",
    "\n",
    "    if len(features_list) == 0:\n",
    "        return(pop_class(train))\n",
    "\n",
    "    min_gini = 1\n",
    "    best_feature = \"\"\n",
    "    best_split = 0\n",
    "\n",
    "    # Iterate through each feature and split-value combination, find the \n",
    "    # combination that yields the lowest gini score.\n",
    "    for feature in features_list:\n",
    "        possible_splits = [\n",
    "            train[feature].quantile(0),\n",
    "            train[feature].quantile(0.25), \n",
    "            train[feature].quantile(0.5),\n",
    "            train[feature].quantile(0.75),\n",
    "            train[feature].quantile(1)\n",
    "        ]\n",
    "        for split in possible_splits:\n",
    "            if (gini_score(feature, train, split) <= min_gini):\n",
    "                min_gini = gini_score(feature, train, split)\n",
    "                best_feature = feature\n",
    "                best_split = split\n",
    "\n",
    "    # We have now used up this classifier, remove it from the feature list\n",
    "    features_list.remove(best_feature)\n",
    "    \n",
    "\n",
    "    split_value = best_split\n",
    "\n",
    "    # Using the best_feature, and its split value, compare it to the test data\n",
    "    # to determine whether to go left or right in the tree\n",
    "    # Split the data like normal, but if we move into a sub-branch of the training\n",
    "    # data that is empty, then we move into the opposite sub-branch\n",
    "    if test[best_feature] <= split_value and len(train.loc[train[best_feature] <= split_value]) > 0:\n",
    "        return(make_tree(features_list, train.loc[train[best_feature] <= split_value], test))\n",
    "    else:\n",
    "        if (len(train.loc[train[best_feature] > split_value]) > 0):\n",
    "            return(make_tree(features_list, train.loc[train[best_feature] > split_value], test))\n",
    "        else:\n",
    "            return(make_tree(features_list, train.loc[train[best_feature] <= split_value], test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Desicion Tree on One Row of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select an index for a row of test data\n",
    "rand_index = random.sample(list(df_test.index), 1)[0]\n",
    "\n",
    "test_1 = df_test.loc[rand_index]\n",
    "features_list = ['Employed', 'Bank Balance', 'Annual Salary']\n",
    "\n",
    "# Output format: (Prediction, actual)\n",
    "make_tree(features_list, df_train, test_1), test_1['Defaulted?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We <u>correctly</u> predicted that this person did not default on their loan! Let us expand.\n",
    "\n",
    "### Testing the Decision Tree on All Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.80%\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "\n",
    "# Count the number of correct classifications for all test data\n",
    "for i in list(df_test.index):\n",
    "    features_list = ['Employed', 'Bank Balance', 'Annual Salary']\n",
    "    test = df_test.loc[i]\n",
    "    if(make_tree(features_list, df_train, test) == test['Defaulted?']):\n",
    "        correct_count += 1\n",
    "\n",
    "accuracy = (correct_count / len(df_test)) * 100\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our decision tree algorithm yields a classification accuracy of 96.80%.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree to Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(features_list, train, test, num_trees, sample_size):\n",
    "    \"\"\"\n",
    "    This function generates 'num_trees' number of decision trees. Each tree \n",
    "    is created from a sub-group of training data by sampling randomly \n",
    "    (with replacement) 'sample_size' number of times. \n",
    "\n",
    "    Given the list of features, a sub-group of training data, and a single \n",
    "    test datapoint, each tree makes its classification. The most popular \n",
    "    classification made by all the trees is returned as the final \n",
    "    classification of the random forest.  \n",
    "    \"\"\"\n",
    "    classifications = []\n",
    "\n",
    "    for i in range(num_trees):\n",
    "        classifications.append(make_tree(\n",
    "            features_list, \n",
    "            train.sample(n=sample_size, replace=True), \n",
    "            test\n",
    "        ))\n",
    "    \n",
    "    return(Counter(classifications).most_common(1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Random Forest on All Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create our random forests such that each forest contains 3 decision trees, each of which are trained on 40 data points. We select a low number like 40 so that we do not overtrain our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.80%\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "\n",
    "# Count the number of correct classifications for all test data\n",
    "for i in range(len(df_test)):\n",
    "    features_list = ['Employed', 'Bank Balance', 'Annual Salary']\n",
    "    if (random_forest(features_list, df_train, df_test.iloc[i], 3, 40) == df_test.iloc[i]['Defaulted?']):\n",
    "        correct_count += 1\n",
    "\n",
    "accuracy = (correct_count / len(df_test)) * 100\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our random forest algorithm yields a classification accuracy of 96.80%.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the implementation of this random forest model by including a pruning step. For instance, using reduced error pruning, we can remove each of the 3 features individually, checking the accuracy cost for each removal. Given that there is a small number of features available, pruning is not a crucial step for this specific application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "213c7e2037ef6b2a20c19d597b02c10a1d7071ecb66c3155fc7058d00ffab593"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
